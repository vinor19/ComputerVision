{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f622c82",
   "metadata": {},
   "source": [
    "# ITCS 4152/5152 Assignment1\n",
    "**Due date: 11:59 pm EST on Sep 30, 2022 (Fri.)**\n",
    "\n",
    "---\n",
    "In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your UNC Charlotte (*.uncc.edu) account for coding and Google Drive to save your results.\n",
    "\n",
    "\n",
    "## Google Colab Tutorial\n",
    "---\n",
    "Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n",
    "\n",
    "Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n",
    "\n",
    "\n",
    "## Local Machine Prerequisites\n",
    "---\n",
    "Since we are using Google Colab, all the code is run on the server environment where lots of libraries or packages have already been installed. In case of missing \n",
    " libraries or if you want to install them in your local machine, below are the links for installation.\n",
    "* **Install Python 3.6**: https://www.python.org/downloads/ or use Anaconda (a Python distribution) at https://docs.continuum.io/anaconda/install/. Below are some materials and tutorials which you may find useful for learning Python if you are new to Python.\n",
    "  - https://docs.python.org/3.6/tutorial/index.html\n",
    "  - https://www.learnpython.org/\n",
    "  - http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html\n",
    "  - http://www.scipy-lectures.org/advanced/image_processing/index.html\n",
    "\n",
    "\n",
    "* **Install Python packages**: install Python packages: `numpy`, `matplotlib`, `opencv-python` using pip, for example:\n",
    "```\n",
    "pip install numpy matplotlib opencv-python\n",
    "``` \n",
    "\tNote that when using “pip install”, make sure that the version you are using is python3. Below are some commands to check which python version it uses in you machine. You can pick one to execute:\n",
    "  \n",
    "```  \n",
    "    pip show pip\n",
    "    pip --version\n",
    "    pip -V\n",
    "```\n",
    "\n",
    "Incase of wrong version, use pip3 for python3 explictly.\n",
    "\n",
    "* **Install Jupyter Notebook**: follow the instructions at http://jupyter.org/install.html to install Jupyter Notebook and familiarize yourself  with it. *After you have installed Python and Jupyter Notebook, please open the notebook file 'assignment1.ipynb' with your Jupyter Notebook and do your homework there.*\n",
    "\n",
    "\n",
    "## Example\n",
    "---\n",
    "Please read through the following examples where we apply image thresholding to an image. This example is desinged to help you get familiar with the basics of Python and routines of OpenCV. This part is for your exercises only, you do not need to submit anything from this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage, misc\n",
    "from IPython.display import display, Image\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dda765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount your google drive where you've saved your assignment folder\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e23e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '------' with the path such that \"ITCS_4152_5152_assignment1\" is your working directory\n",
    "cd '/content/gdrive/My Drive/------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for image thresholding\n",
    "def imThreshold(img, threshold, maxVal):\n",
    "    assert len(img.shape) == 2 # input image has to be gray\n",
    "    \n",
    "    height, width = img.shape\n",
    "    bi_img = np.zeros((height, width), dtype=np.uint8)\n",
    "    for x in range(height):\n",
    "        for y in range(width):\n",
    "            if img.item(x, y) > threshold:\n",
    "                bi_img.itemset((x, y), maxVal)\n",
    "                \n",
    "    return bi_img\n",
    "\n",
    "# read the image for local directory (same with this .ipynb) \n",
    "img = cv2.imread('SourceImages/IloveCV.png')\n",
    "\n",
    "# convert a color image to gray\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# image thresholding using global tresholder\n",
    "img_bi = imThreshold(img_gray, 127, 255)\n",
    "\n",
    "# Be sure to convert the color space of the image from\n",
    "# BGR (Opencv) to RGB (Matplotlib) before you show a \n",
    "# color image read from OpenCV\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('color image')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img_gray, 'gray')\n",
    "plt.title('gray image')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_bi, 'gray')\n",
    "plt.title('binarized image')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89bf7d7",
   "metadata": {},
   "source": [
    "## Start of Assignment1\n",
    "---\n",
    "This assignment is divided into two parts each carrying equal weightage of points (pts). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e944e",
   "metadata": {},
   "source": [
    "## Task1 - Description\n",
    "---\n",
    "There are five basic image processing problems in total with specific instructions for each of them. Be sure to read **Submission Guidelines** at the end of this file. They are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1792a2f",
   "metadata": {},
   "source": [
    "## Problems\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc1f09",
   "metadata": {},
   "source": [
    "- **Problem T1.1 Gaussian filter {10 pts}:** \n",
    "\n",
    "  (a) Write a function that takes two arguments, a width parameter and a sigma parameter, and returns a 2D array containing a Gaussian kernel of the desired dimension and variance. The peak of the Gaussian should be in the center of the array. Make sure to normalize the kernel such that the sum of all the elements in the array is 1. Use this function and the OpenCV’s `filter2D` routine to convolve the image and noisy image arrays with a 5x5 Gaussian kernel of sigma=1 and a 11x11 Gaussian kernel of sigma=2. There will be four output images from this problem, namely, image convolved with 5x5, and 11x11, noisy image convolved with 5x5, and 11x11. \n",
    "\n",
    "  (b) Write a function that takes an image and its noisy version, and return the Peak Signal-to-Noise Ratio (PSNR) value. Refer to https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio for its mathematical form. Pay attention that you might need to do convertion between different data types.\n",
    "\n",
    "  Once you fill in and run the codes, the outputs will be saved under `Results` folder. Also images will be displayed in the notebook with PSNR marked in the titles. Please only fill in the missing part as indicated by `##########--WRITE YOUR CODE HERE--##########` and do not modify other parts. The grading for this question will be based on the ground-truth PSNR and your results.\n",
    "\n",
    "  Lena image is credit to https://www.ece.rice.edu/~wakin/images/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genGaussianKernel(width, sigma):    \n",
    "    ##########--WRITE YOUR CODE HERE--##########\n",
    "\n",
    "\n",
    "\n",
    "    ##########-------END OF CODE-------##########\n",
    "    return kernel_2d\n",
    "\n",
    "def PSNR(img, img_noise):\n",
    "    ##########--WRITE YOUR CODE HERE--##########\n",
    "\n",
    "\n",
    "\n",
    "    ##########-------END OF CODE-------##########\n",
    "    return PSNR\n",
    " \n",
    "# Function to generate image with Gaussian noise\n",
    "def addGaussianNoise(img, mean, std, seed=0):\n",
    "    # suppose the input image is 2D gray image\n",
    "    np.random.seed(seed)\n",
    "    img_noise = img.astype(np.float32)/255.\n",
    "    noise = np.random.normal(mean, std, img.shape)\n",
    "    img_noise = img_noise + noise\n",
    "    img_noise = np.clip(img_noise, 0., 1.)\n",
    "    img_noise = (img_noise*255).astype(np.uint8)\n",
    "    return img_noise\n",
    "\n",
    "# Load images\n",
    "img = cv2.imread('SourceImages/lena512.bmp', 0)\n",
    "\n",
    "# Add Gaussian Noise\n",
    "img_gnoise = addGaussianNoise(img, 0, 0.1)\n",
    "PSNR_img_gnoise = PSNR(img, img_gnoise)\n",
    "\n",
    "# Generate Gaussian kernels\n",
    "kernel_1 = genGaussianKernel(5, 1)  # 5x5 kernel of sigma=1\n",
    "kernel_2 = genGaussianKernel(11, 2)  # 11x11 kernel of sigma=2\n",
    "\n",
    "# Convolve kernel with (noisy) image\n",
    "img_kernel1 = cv2.filter2D(img, -1, kernel_1)\n",
    "img_kernel2 = cv2.filter2D(img, -1, kernel_2)\n",
    "img_gnoise_kernel1 = cv2.filter2D(img_gnoise, -1, kernel_1)\n",
    "img_gnoise_kernel2 = cv2.filter2D(img_gnoise, -1, kernel_2)\n",
    "\n",
    "PSNR_img_gnoise_kernel1 = PSNR(img, img_gnoise_kernel1)\n",
    "PSNR_img_gnoise_kernel2 = PSNR(img, img_gnoise_kernel2)\n",
    "\n",
    "# Write result images\n",
    "if not os.path.exists(\"Results\"):\n",
    "  os.makedirs(\"Results\")\n",
    "cv2.imwrite(\"Results/P1_01.jpg\", img_kernel1)\n",
    "cv2.imwrite(\"Results/P1_02.jpg\", img_kernel2)\n",
    "cv2.imwrite(\"Results/P1_03.jpg\", img_gnoise_kernel1)\n",
    "cv2.imwrite(\"Results/P1_04.jpg\", img_gnoise_kernel2)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img, 'gray')\n",
    "plt.title('Image: original')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img_kernel1, 'gray')\n",
    "plt.title('Image: 5x5 kernel of sigma=1')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_kernel2, 'gray')\n",
    "plt.title('Image: 11x11 kernel of sigma=2')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_gnoise, 'gray')\n",
    "plt.title('Noisy image: original\\n PSNR:{0:.2f}'.format(PSNR_img_gnoise))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img_gnoise_kernel1, 'gray')\n",
    "plt.title('Noisy image: 5x5 kernel of sigma=1\\n PSNR:{0:.2f}'\n",
    "                        .format(PSNR_img_gnoise_kernel1))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_gnoise_kernel2, 'gray')\n",
    "plt.title('Noisy image: 11x11 kernel of sigma=2\\n PSNR:{0:.2f}'\n",
    "                        .format(PSNR_img_gnoise_kernel2))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aafe20",
   "metadata": {},
   "source": [
    "- **Problem T1.2 Median filter {10 pts}:** \\\\\n",
    "(a) Write a function to generate an image with salt and pepper noise. The function takes three arguments, the input image, the probability that a pixel location has salt-pepper noise and a random seed for repitability. A simple implementation can be to select pixel locations with probability 'p' where noise occurs and then with equal probability set the pixel value at those location to be 0 or 255.(**Hint: Use np.random.uniform and np.random.choice**) \n",
    "\n",
    " (b) Write a function to implement a median filter. The function takes two arguments, an image and a window size (if window size is 'k', then a kxk window is used to determine the median pixel value at a location) and returns the output image. **Do not** use any inbuilt library (like scipy.ndimage_filter) to directly generate the result. (**Hint: Use cv2.copyMakeBorder to add borders**)  \n",
    "\n",
    " You need to generate two noisy images corrupted by salt-and-pepper noise with noise probability equals 0.1 and 0.2. And then use median filter of window size=5 to denoise the two noisy images. You also need to denoise them with Gaussian filters. Try different parameters of Gaussian filters to get as higher PSNR as possible. Fill in all the missing parts and do not modify others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f820ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate image with salt and pepper noise\n",
    "def addSaltPepperNoise(img, prob = 0.1, seed=0):\n",
    "  # suppose the input image is 2D gray image\n",
    "  # with probability=prob each pixel is replaced with a pepper(0)\n",
    "  # or a salt(255) in equal chance\n",
    "  np.random.seed(seed)  \n",
    "  ##########--WRITE YOUR CODE HERE--##########\n",
    "\n",
    "\n",
    "\n",
    "  ##########-------END OF CODE-------##########\n",
    "  return img_noise\n",
    "  \n",
    "# Function to apply median filter (window size kxk) on the input image  \n",
    "def medianFilter(img, window_size = 5):\n",
    "  ##########--WRITE YOUR CODE HERE--##########  \n",
    "\n",
    "\n",
    "\n",
    "  ##########-------END OF CODE-------##########\n",
    "  return img_filtered\n",
    "  \n",
    "img_spnoise_p1 = \n",
    "PSNR_img_spnoise_p1 = \n",
    "\n",
    "img_spnoise_p1_MedianFilter =\n",
    "PSNR_img_spnoise_p1_MedianFilter = \n",
    "\n",
    "img_spnoise_p2 = \n",
    "PSNR_img_spnoise_p2 = \n",
    "\n",
    "img_spnoise_p2_MedianFilter = \n",
    "PSNR_img_spnoise_p2_MedianFilter = \n",
    "\n",
    "# Generate Gaussian kernels\n",
    "kernel_1 =  \n",
    "kernel_2 = \n",
    "\n",
    "# Convolve filters with image and noisy image\n",
    "img_spnoise_p1_GaussianFilter = cv2.filter2D(img_spnoise_p1, -1, kernel_1)\n",
    "PSNR_img_spnoise_p1_GaussianFilter = PSNR(img, img_spnoise_p1_GaussianFilter)\n",
    "\n",
    "img_spnoise_p2_GaussianFilter = cv2.filter2D(img_spnoise_p2, -1, kernel_2)\n",
    "PSNR_img_spnoise_p2_GaussianFilter = PSNR(img, img_spnoise_p2_GaussianFilter)\n",
    "\n",
    "cv2.imwrite(\"Results/P1_05.jpg\", img_spnoise_p1_MedianFilter)    \n",
    "cv2.imwrite(\"Results/P1_06.jpg\", img_spnoise_p1_GaussianFilter)    \n",
    "cv2.imwrite(\"Results/P1_07.jpg\", img_spnoise_p2_MedianFilter)    \n",
    "cv2.imwrite(\"Results/P1_08.jpg\", img_spnoise_p2_GaussianFilter)  \n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img, 'gray')\n",
    "plt.title('Image: original')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_spnoise_p1, 'gray')\n",
    "plt.title('Noise image: salt and pepper (prob = 0.1)\\n PSNR:{0:.2f}'\n",
    "                .format(PSNR_img_spnoise_p1))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img_spnoise_p1_MedianFilter, 'gray')\n",
    "plt.title('Noise image: median filter (prob = 0.1)\\n PSNR:{0:.2f}'\n",
    "                .format(PSNR_img_spnoise_p1_MedianFilter))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_spnoise_p1_GaussianFilter, 'gray')\n",
    "plt.title('Noise image: Gaussian filter (prob = 0.1)\\n PSNR:{0:.2f}'\n",
    "                .format(PSNR_img_spnoise_p1_GaussianFilter))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_spnoise_p2, 'gray')\n",
    "plt.title('Noise image: salt and pepper (prob = 0.2)\\n PSNR:{0:.2f}'\n",
    "                .format(PSNR_img_spnoise_p2))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img_spnoise_p2_MedianFilter, 'gray')\n",
    "plt.title('Noise image: median filter (prob = 0.2)\\n PSNR:{0:.2f}'\n",
    "                .format(PSNR_img_spnoise_p2_MedianFilter))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_spnoise_p2_GaussianFilter, 'gray')\n",
    "plt.title('Noise image: Gaussian filter (prob = 0.2)\\n PSNR:{0:.2f}'\n",
    "                  .format(PSNR_img_spnoise_p2_GaussianFilter))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c43585",
   "metadata": {},
   "source": [
    "- **Problem T1.3 Separable filters {10 pts}:** The Gaussian kernel is separable, which means that convolution with a 2D Gaussian can be accomplished by convolving the image with two 1D Gaussians, one in the x direction and the other one in the y direction. Perform an 11x11 convolution with sigma = 3 from question 1 using this scheme. You can still use `filter2D` to convolve the images with each of the 1D kernels. Verify that you get the same results with what you did with 2D kernels by computing the difference image between the results from the two methods. This difference image should be close to black. Include your code and results in your colab Notebook file. There is no output image from this part. Be sure to display the result images in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf355b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genGausKernel1D(length, sigma):\n",
    "    ##########--WRITE YOUR CODE HERE--##########    \n",
    "\n",
    "\n",
    "\n",
    "    ##########-------END OF CODE-------##########\n",
    "    return kernel_1d\n",
    "\n",
    "\n",
    "# Generate two 1d kernels here\n",
    "width = 11\n",
    "sigma = 3\n",
    "kernel_x =  \n",
    "kernel_y = \n",
    "\n",
    "# Generate a 2d 11x11 kernel with sigma of 3 here as before\n",
    "kernel_2d = \n",
    "\n",
    "# Convolve with img_gnoise\n",
    "img_gnoise_kernel1d_x =  \n",
    "img_gnoise_kernel1d_xy =  \n",
    "img_gnoise_kernel2d = \n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_gnoise, 'gray')\n",
    "plt.title('Noise image: original')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img_gnoise_kernel1d_x, 'gray')\n",
    "plt.title('Noise image: Gaussian filter 1d (x)')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_gnoise_kernel1d_xy, 'gray')\n",
    "plt.title('Noise image: Gaussian filter 1d (x->y)')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img_gnoise_kernel2d, 'gray')\n",
    "plt.title('Noise image: Gaussian filter 2d')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Compute the difference array here\n",
    "img_diff =  np.abs((img_gnoise_kernel1d_xy.astype(np.float32)\n",
    "       -img_gnoise_kernel2d.astype(np.float32))).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img_diff, 'gray', vmin=0, vmax=255)\n",
    "plt.title('Difference image\\n Max abs difference={0:d}'\n",
    "                        .format(np.max(img_diff)))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e8fbe",
   "metadata": {},
   "source": [
    "- **Problem T1.4 Difference of Gaussians {10 pts}:** Difference of Gaussians (DoG) is a feature enhancement algorithm. You can obtain a DoG by subtracting a Gaussian filter of sigma=K\\*s from a Gaussian filter of sigma=s. In this question, we will use K=1.6 and sigma=2. Plot the DoG kernel using the `Matplotlib` function `plot`. Use the `Matplotlib` function `plot_surface` to generate a 3D plot of DoG. Include your code and results in your Colab Notebook file. Apply the filter to the noisy image generated in the previous question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your Gaussian kernel\n",
    "Gaussian_kernel_1 = \n",
    "Gaussian_kernel_2 = \n",
    "\n",
    "# Create your Difference of Gaussian\n",
    "DoG = \n",
    "\n",
    "# Plot Gaussian and Laplacian\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(Gaussian_kernel_1, interpolation='none', cmap=cm.jet)\n",
    "plt.title('Gaussian kernel 1')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(Gaussian_kernel_2, interpolation='none', cmap=cm.jet)\n",
    "plt.title('Gaussian kernel 2')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(DoG, interpolation='none', cmap=cm.jet)\n",
    "plt.title('2D Difference of Gaussian')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot the 3D figure of DoG\n",
    "##########--WRITE YOUR CODE HERE--##########\n",
    "\n",
    "\n",
    "\n",
    "##########-------END OF CODE-------##########\n",
    "\n",
    "img_gnoise_DoG = \n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_gnoise, 'gray')\n",
    "plt.title('Noise image: original')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img_gnoise_DoG, 'gray')\n",
    "plt.title('Noise image: filtered with DoG')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "cv2.imwrite(\"Results/P3_01.jpg\", img_gnoise_DoG);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64394c74",
   "metadata": {},
   "source": [
    "- **Problem T1.5 Histogram equalization {10 pts}:** Refer to Szeliski's book on section 3.4.1, and within that section to eqn 3.9 for more information on histogram equalization. Getting the histogram of a grayscale image is incredibly easy with python. A histogram is a vector of numbers. Once you have the histogram, you can get the cumulative distribution function (CDF) from it. Then all you have left is to find the mapping from each value [0,255] to its adjusted value (just using the CDF basically). **DO NOT** use **cv2.equalizeHist()** directly to solve the exercise! We will expect to see in your code that you get the PDF and CDF, and that you manipulate the pixels directly (avoid a for loop, though). The ground-truth image is credit to https://github.com/yinhaoz/denoising-fluorescence. It is a real Fluorescence Microscopy Image with three R/G/B channels. You need to transform it into HSV space before doing histogram equalization. There will be one output image from this part which is the histogram equalized image. It will be compared against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_equalization(img):\n",
    "    ##########--WRITE YOUR CODE HERE--##########    \n",
    "\n",
    "\n",
    "\n",
    "    ##########-------END OF CODE-------##########\n",
    "    return img_hist_eq\n",
    "\n",
    "# Read in input images\n",
    "imgB = cv2.imread('SourceImages/BPAE_Cells.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "# Histogram equalization\n",
    "imgB_hist_eq = histogram_equalization(imgB)\n",
    "\n",
    "# Plot results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(imgB, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Image: original')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot results\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(imgB_hist_eq, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Image: histogram equalization')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Write out results\n",
    "cv2.imwrite(\"Results/P4_01.jpg\", imgB_hist_eq);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3091692",
   "metadata": {},
   "source": [
    "## Task2 - Description\n",
    "---\n",
    "In this part of the assignment you will experiment with SIFT features for scene matching. You will work with the SIFT tutorial and code from the University of Toronto. In the compressed homework file, you will find the tutorial document (tutSIFT04.pdf) and a paper from the International Journal of Computer Vision (ijcv04.pdf) describing SIFT. Although the tutorial document assumes matlab implemention, you should still be able to follow the technical details in it. In addition, you are **STRONGLY** encouraged to read this paper unless you’re already quite familiar with matching and recognition using SIFT.\n",
    "\n",
    "There are 2 problems in this task with a total of 50 points. Two bonus questions with extra 5 and 15 points are provided under problem 1 and 2 respectively. Thus, the maximum points you may earn from this homework is 100 + 20 = 120 points. Be sure to read **Submission Guidelines** below. They are important.\n",
    "\n",
    "\n",
    "\n",
    "## Using SIFT in OpenCV 3.x.x in Local Machine\n",
    "---\n",
    "Feature descriptors like SIFT and SURF are no longer included in OpenCV since version 3. This section provides instructions on how to use SIFT for those who use OpenCV 3.x.x. If you are using OpenCV 2.x.x then you are all set, please skip this section. Read this if you are curious about why SIFT is removed https://www.pyimagesearch.com/2015/07/16/where-did-sift-and-surf-go-in-opencv-3/.\n",
    "\n",
    "If you want to use SIFT in your local machine, one simple way to use the OpenCV in-built function `SIFT` is to switch back to version 2.x.x, but if you want to keep using OpenCV 3.x.x, do the following:\n",
    "1. uninstall your original OpenCV package\n",
    "2. install opencv-contrib-python using pip (pip is a Python tool for installing packages written in Python), please find detailed instructions at https://pypi.python.org/pypi/opencv-contrib-python\n",
    "\n",
    "After you have your OpenCV set up, you should be able to use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n",
    "\n",
    "## Using SIFT in OpenCV 3.x.x in Colab (RECOMMENDED)\n",
    "---\n",
    "The default version of OpenCV in Colab is 3.4.3. If we use SIFT method directly, typically we will get this error message:\n",
    "\n",
    "```\n",
    "error: OpenCV(3.4.3) /io/opencv_contrib/modules/xfeatures2d/src/sift.cpp:1207: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'create'\n",
    "\n",
    "```\n",
    "\n",
    "One simple way to use the OpenCV in-built function `SIFT` in Colab is to switch the version to the one from 'contrib'. Below is an example of switching OpenCV version:\n",
    "\n",
    "1. Run the following command in one section in Colab, which has already been included in this assignment:\n",
    "```\n",
    "pip install opencv-contrib-python==3.4.2.16\n",
    "```\n",
    "2. Restart runtime by\n",
    "```\n",
    "Runtime -> Restart Runtime\n",
    "```\n",
    "\n",
    "Then you should be able to use use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n",
    "\n",
    "## Some Resources\n",
    "---\n",
    "In addition to the tutorial document, the following resources can definitely help you in this homework:\n",
    "- http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html\n",
    "- http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html\n",
    "- http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html?highlight=sift#cv2.SIFT\n",
    "- http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install the OpenCV version from 'contrib'\n",
    "pip install opencv-contrib-python==3.4.2.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14125312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages here\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(cv2.__version__) # verify OpenCV version\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount your google drive where you've saved your assignment folder\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '------' with the path such that \"CSE527-22S-HW2\" is your working directory\n",
    "# cd '/content/gdrive/My Drive/------'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e6490",
   "metadata": {},
   "source": [
    "## Problem T2.1: Match transformed images using SIFT features\n",
    "{25 points + bonus 5} You will transform a given image, and match it back to the original image using SIFT keypoints. \n",
    "\n",
    "- **Step 1 (5pt)**. Use the function from SIFT class to detect keypoints from the given image. Plot the image with keypoints scale and orientation overlaid.\n",
    "\n",
    "- **Step 2 (5pt)**. Rotate your image clockwise by 45 degrees with the `cv2.warpAffine` function. Extract SIFT keypoints for this rotated image and plot the rotated picture with keypoints scale and orientation overlaid just as in step 1.\n",
    "\n",
    "- **Step 3 (10pt)**. Match the SIFT keypoints of the original image and the rotated imag using the `knnMatch` function in the `cv2.BFMatcher` class. Discard bad matches using the ratio test proposed by D.Lowe in the SIFT paper. Use **0.1** as the ratio in this assignment. Note that this is for display purpose only. Draw the filtered good keypoint matches on the image and display it. The image you draw should have two images side by side with matching lines across them.\n",
    "\n",
    "- **Step 4 (5pt)**. Use the RANSAC algorithm to find the affine transformation from the rotated image to the original image. You are **not** required to implement the RANSAC algorithm yourself, instead you could use the `cv2.findHomography` function (set the 3rd parameter `method` to `cv2.RANSAC`) to compute the transformation matrix. Transform the rotated image back using this matrix and the `cv2.warpPerspective` function. Display the recovered image.\n",
    "\n",
    "- **Bonus (5pt)**. You might have noticed that the rotated image from step 2 is cropped. Rotate the image without any cropping and you will be awarded an extra 5 points.\n",
    "\n",
    "Hints: In case of too many matches in the output image, use the ratio of 0.1 to filter matches.\n",
    "\n",
    "The image is a duplicate of *Table in front of window* by Pablo Picasso. See https://www.pablopicasso.org/ for more stories about Pablo Picasso and https://www.wikiart.org/en/pablo-picasso/table-in-front-of-window-1919 for more information about this work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cdaf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawMatches(img1, kp1, img2, kp2, matches):\n",
    "    \"\"\"\n",
    "    My own implementation of cv2.drawMatches as OpenCV 2.4.9\n",
    "    does not have this function available but it's supported in\n",
    "    OpenCV 3.0.0\n",
    "\n",
    "    This function takes in two images with their associated \n",
    "    keypoints, as well as a list of DMatch data structure (matches) \n",
    "    that contains which keypoints matched in which images.\n",
    "\n",
    "    An image will be produced where a montage is shown with\n",
    "    the first image followed by the second image beside it.\n",
    "\n",
    "    Keypoints are delineated with circles, while lines are connected\n",
    "    between matching keypoints.\n",
    "\n",
    "    img1,img2 - Grayscale or Color images\n",
    "    kp1,kp2 - Detected list of keypoints through any of the OpenCV keypoint \n",
    "              detection algorithms\n",
    "    matches - A list of matches of corresponding keypoints through any\n",
    "              OpenCV keypoint matching algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a new output image that concatenates the two images together\n",
    "    # (a.k.a) a montage\n",
    "    rows1 = img1.shape[0]\n",
    "    cols1 = img1.shape[1]\n",
    "    rows2 = img2.shape[0]\n",
    "    cols2 = img2.shape[1]\n",
    "\n",
    "    # Create the output image\n",
    "    # The rows of the output are the largest between the two images\n",
    "    # and the columns are simply the sum of the two together\n",
    "    # The intent is to make this a colour image, so make this 3 channels\n",
    "    out = np.zeros((max([rows1,rows2]),cols1+cols2,3), dtype='uint8')\n",
    "\n",
    "    # Place the first image to the left\n",
    "    # stack if the inputs are gray images\n",
    "    if len(img1.shape) == 2:\n",
    "      img1 = np.dstack([img1, img1, img1])\n",
    "    if len(img2.shape) == 2:\n",
    "      img2 = np.dstack([img2, img2, img2]) \n",
    " \n",
    "    out[:rows1, :cols1, :] = img1\n",
    "\n",
    "    # Place the next image to the right of it\n",
    "    out[:rows2, cols1:, :] = img2\n",
    "\n",
    "    # For each pair of points we have between both images\n",
    "    # draw circles, then connect a line between them\n",
    "    for mat in matches:\n",
    "        # Get the matching keypoints for each of the images\n",
    "        img1_idx = mat.queryIdx\n",
    "        img2_idx = mat.trainIdx\n",
    "\n",
    "        # x - columns\n",
    "        # y - rows\n",
    "        (x1,y1) = kp1[img1_idx].pt\n",
    "        (x2,y2) = kp2[img2_idx].pt\n",
    "\n",
    "        # Draw a small circle at both co-ordinates\n",
    "        # radius 4\n",
    "        # colour blue\n",
    "        # thickness = 1\n",
    "        cv2.circle(out, (int(x1),int(y1)), 4, (255, 0, 0), 1)   \n",
    "        cv2.circle(out, (int(x2)+cols1,int(y2)), 4, (255, 0, 0), 1)\n",
    "\n",
    "        # Draw a line in between the two points\n",
    "        # thickness = 1\n",
    "        # colour blue\n",
    "        cv2.line(out, (int(x1),int(y1)), (int(x2)+cols1,int(y2)), (0,255,0), 2)\n",
    "    # Also return the image if you'd like a copy\n",
    "    return out\n",
    "\n",
    "# Read image\n",
    "img_input = cv2.imread('SourceImages/Picasso.png')\n",
    "\n",
    "##########--WRITE YOUR CODE HERE--##########\n",
    "# initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp, des = \n",
    "\n",
    "# Draw keypoints on the image\n",
    "# use cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS flag\n",
    "res1 = \n",
    "\n",
    "# rotate image\n",
    "# use cv2.warpAffine to rotate image\n",
    "img_input_rot =    \n",
    "\n",
    "# find the keypoints and descriptors on the rotated image\n",
    "sift_rot = cv2.xfeatures2d.SIFT_create()\n",
    "kp_rot, des_rot = \n",
    "\n",
    "# Draw keypoints on the rotated image\n",
    "# use cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS flag\n",
    "res2 = \n",
    "##########-------END OF CODE-------##########\n",
    "\n",
    "# Plot result images\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(res1, cv2.COLOR_BGR2RGB));\n",
    "plt.title('original img')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(res2, cv2.COLOR_BGR2RGB));\n",
    "plt.title('rotated img')\n",
    "plt.axis('off')\n",
    "\n",
    "##########--WRITE YOUR CODE HERE--##########\n",
    "# compute feature matching\n",
    "# use the knnMatch function in the cv2.BFMatcher class\n",
    "\n",
    "\n",
    "# Apply ratio test to keep good matches; ratio=0.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# draw matching results with the given drawMatches function\n",
    "res3 = \n",
    "##########-------END OF CODE-------##########\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.imshow(cv2.cvtColor(res3, cv2.COLOR_BGR2RGB));\n",
    "plt.title('matching')\n",
    "plt.axis('off')\n",
    "\n",
    "##########--WRITE YOUR CODE HERE--##########\n",
    "# find perspective transform matrix using RANSAC\n",
    "# use cv2.findHomography\n",
    "  \n",
    "\n",
    "# mapping rotataed image back with the calculated rotation matrix\n",
    "# use cv2.warpPerspective\n",
    "\n",
    "\n",
    "res4 = \n",
    "##########-------END OF CODE-------##########\n",
    "\n",
    "\n",
    "# plot result images\n",
    "plt.figure(figsize=(14,8));\n",
    "plt.subplot(1, 2, 1);\n",
    "plt.imshow(cv2.cvtColor(img_input, cv2.COLOR_BGR2RGB));\n",
    "plt.title('original img');\n",
    "plt.axis('off');\n",
    "\n",
    "plt.subplot(1, 2, 2);\n",
    "plt.imshow(cv2.cvtColor(res4, cv2.COLOR_BGR2RGB));\n",
    "plt.title('recovered img');\n",
    "plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff81a12",
   "metadata": {},
   "source": [
    "## Problem T2.2: Scene stitching with SIFT features\n",
    "{30 points + 15 bonus} You will match and align between different views of a scene with SIFT features. \n",
    "\n",
    "Use `cv2.copyMakeBorder` function to pad the center image with zeros into a larger size. Extract SIFT features for all images and go through the same procedures as you did in problem 1. Your goal is to find the affine transformation between the two images and then align one of your images to the other using `cv2.warpPerspective`. Use the `cv2.addWeighted` function (or your own implementation) to blend the aligned images and show the stitched result. Examples can be found at http://docs.opencv.org/trunk/d0/d86/tutorial_py_image_arithmetics.html.\n",
    "Use parameters **0.5 and 0.5** for alpha blending.\n",
    "\n",
    "- **Step 1 (15pt)**. Compute the transformation from the right image to the center image. Warp the right image with the computed transformation. Stitch the center and right images with alpha blending. Display the SIFT feature matching between the center and right images like you did in problem 1. Display the stitched result (center and right image).\n",
    "\n",
    "- **Step 2 (10pt)** Compute the transformation from the left image to the stitched image from step 1. Warp the left image with the computed transformation. Stich the left and result images from step 1 with alpha blending. Display the SIFT feature matching between the result image from step 1 and the left image like what you did in problem 1. Display the final stitched result (all three images).\n",
    "\n",
    "- **Bonus (15pt)**. Instead of using `cv2.addWeighted` to do the blending, implement Laplacian Pyramids to blend the two aligned images. Tutorials can be found at http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_pyramids/py_pyramids.html. Display the stitched result (center and right image) and the final stitched result (all three images) with laplacian blending instead of alpha blending.\n",
    "\n",
    "Note that for the resultant stitched image, some might have different intensity in the overlapping and other regions, namely the overlapping region looks brighter or darker than others. To get full credit, the final image should have uniform illumination.\n",
    "\n",
    "Hints: You need to find the warping matrix between images with the same mechanism from problem 1. You will need as many reliable matches as possible to find a good homography so DO NOT use 0.1 here. A suggested value would be 0.75 in this case.\n",
    "\n",
    "When you warp the image with cv2.warpPerspective, an important trick is to pass in the correct parameters so that the warped image has the same size with the padded_center image. Once you have two images with the same size, find the overlapping part and do the blending.\n",
    "\n",
    "The images are the Building of Woodward Hall in UNC Charlotte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6476871",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgCenter = cv2.imread('SourceImages/woodward_m.png', cv2.IMREAD_COLOR)\n",
    "imgRight = cv2.imread('SourceImages/woodward_r.png', cv2.IMREAD_COLOR)\n",
    "imgLeft = cv2.imread('SourceImages/woodward_l.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "# initalize the stitched image as the center image\n",
    "# the following is recommended padding size, DO NOT change it \n",
    "imgCenter = cv2.copyMakeBorder(imgCenter,160,160,400,400,cv2.BORDER_CONSTANT)\n",
    "\n",
    "def alpha_blending(img_A, img_B):    \n",
    "    # Implement alpha_blending, using 0.5 and 0.5 for alphas\n",
    "    ##########--WRITE YOUR CODE HERE--##########\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ##########-------END OF CODE-------##########\n",
    "    return blended\n",
    "    \n",
    "\n",
    "def Laplacian_blending(img_A, img_B, num_levels=5, mask=None):\n",
    "    # Implement Laplacian_blending\n",
    "    # num_levels is the number of levels in the pyramids\n",
    "    # assume mask is float32 [0,1], it has the same size as img_A and img_B\n",
    "    # the mask indicates which parts of img_A or img_B are blended together\n",
    "    # a simple example could be  \n",
    "    # mask = np.hstack([np.zeros([img_A.shape[0],img_A.shape[1]//2,img_A.shape[2]], np.float32),\n",
    "    #  np.ones([img_B.shape[0],img_B.shape[1]//2,img_B.shape[2]], np.float32)])\n",
    "    # but it depends on your choice\n",
    "    # you may even skip this parameter if you always use one-half 0s and one-half 1s like the tutorial\n",
    "    \n",
    "    assert img_A.shape == img_B.shape\n",
    "    assert img_A.shape == mask.shape\n",
    "    ##########--WRITE YOUR CODE HERE--########## \n",
    "    # generate Gaussian pyramids for img_A, img_B and corresponding masks   \n",
    "\n",
    "\n",
    "      \n",
    "    # generate Laplacian pyramids for img_A, img_B and corresponding masks \n",
    "    \n",
    "\n",
    "    \n",
    "    # now blend images with your mask in each level\n",
    "    \n",
    "        \n",
    "\n",
    "    # now reconstruct the blended image\n",
    "    \n",
    "    \n",
    "    ##########-------END OF CODE-------##########\n",
    "    return blended\n",
    "\n",
    "def getTransform(img1, img2):\n",
    "    ##########--WRITE YOUR CODE HERE--##########\n",
    "    # compute sift descriptors\n",
    "\n",
    "\n",
    "    # find all matches\n",
    "\n",
    "\n",
    "    # apply ratio test, use ratio = 0.75\n",
    "\n",
    "        \n",
    "    # draw matches\n",
    "\n",
    "\n",
    "    # find perspective transform matrix using RANSAC\n",
    "   \n",
    "            \n",
    "    ##########-------END OF CODE-------##########\n",
    "    # H is the perspective transform matrix\n",
    "    # img_match is the image returned by drawMatches\n",
    "    return H, img_match\n",
    "\n",
    "def perspective_warping_alpha_blending(imgCenter, imgLeft, imgRight):\n",
    "    ##########--WRITE YOUR CODE HERE--##########\n",
    "    # Get homography from right to center\n",
    "    # img_match_cr is your first output\n",
    "    # call getTransform to get the transformation from the right to the center image\n",
    "\n",
    "    \n",
    "    # Blend center and right\n",
    "    # stitched_cr is your second output, returned by alpha_blending \n",
    "    # call alpha_blending\n",
    "\n",
    "    \n",
    "    # Get homography from left to stitched center_right\n",
    "    # img_match_lcr is your third output\n",
    "    # call getTransform to get the transformation from the left to stitched_cr\n",
    "    \n",
    "\n",
    "    # Blend left and center_right\n",
    "    # stitched_lcr is your fourth output, returned by alpha_blending  \n",
    "    # call alpha_blending\n",
    "\n",
    "\n",
    "    ##########-------END OF CODE-------##########\n",
    "    return img_match_cr, stitched_cr, img_match_lcr, stitched_lcr \n",
    "\n",
    "def perspective_warping_laplacian_blending(imgCenter, imgLeft, imgRight):\n",
    "    ##########--WRITE YOUR CODE HERE--##########\n",
    "    # Get homography from right to center\n",
    "    # call getTransform to get the transformation from the right to the center image\n",
    "\n",
    "    \n",
    "    # Blend center and right\n",
    "    # stitched_cr is your first bonus output, returned by Laplacian_blending \n",
    "    # call Laplacian_blending \n",
    "\n",
    "    \n",
    "    # Get homography from left to stitched center_right\n",
    "    # call getTransform to get the transformation from the left to stitched_cr\n",
    "    \n",
    "    \n",
    "    # Blend left and center_right\n",
    "    # stitched_lcr is your second bonus output, returned by Laplacian_blending\n",
    "    # call Laplacian_blending\n",
    "    \n",
    "    \n",
    "    ##########-------END OF CODE-------##########\n",
    "    return img_match_cr, stitched_cr, img_match_lcr, stitched_lcr \n",
    "\n",
    "\n",
    "img_match_cr, stitched_cr, img_match_lcr, stitched_lcr = perspective_warping_alpha_blending(imgCenter, imgLeft, imgRight)\n",
    "img_match_cr_lap, stitched_cr_lap, img_match_lcr_lap, stitched_lcr_lap = perspective_warping_laplacian_blending(imgCenter, imgLeft, imgRight)\n",
    "        \n",
    "plt.figure(figsize=(15,30));\n",
    "plt.subplot(4, 1, 1);\n",
    "plt.imshow(cv2.cvtColor(img_match_cr, cv2.COLOR_BGR2RGB));\n",
    "plt.title(\"center and right matches\");\n",
    "plt.axis('off');\n",
    "plt.subplot(4, 1, 2);\n",
    "plt.imshow(cv2.cvtColor(stitched_cr, cv2.COLOR_BGR2RGB));\n",
    "plt.title(\"center, right: stitched result\");\n",
    "plt.axis('off');\n",
    "plt.subplot(4, 1, 3);\n",
    "plt.imshow(cv2.cvtColor(img_match_lcr, cv2.COLOR_BGR2RGB));\n",
    "plt.title(\"left and center_right matches\");\n",
    "plt.axis('off');\n",
    "plt.subplot(4, 1, 4);\n",
    "plt.imshow(cv2.cvtColor(stitched_lcr, cv2.COLOR_BGR2RGB));\n",
    "plt.title(\"left, center, right: stitched result\");\n",
    "plt.axis('off');\n",
    "plt.show();\n",
    "\n",
    "plt.figure(figsize=(15,30));\n",
    "plt.subplot(4, 1, 1);\n",
    "plt.imshow(cv2.cvtColor(stitched_cr_lap, cv2.COLOR_BGR2RGB));\n",
    "plt.title(\"Bonus, center, right: stitched result\");\n",
    "plt.axis('off');\n",
    "plt.subplot(4, 1, 2);\n",
    "plt.imshow(cv2.cvtColor(stitched_lcr_lap, cv2.COLOR_BGR2RGB));\n",
    "plt.title(\"Bonus, left, center, right: stitched result\");\n",
    "plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1dd9c8",
   "metadata": {},
   "source": [
    "## Submission guidelines\n",
    "---\n",
    "Please submit a pdf file that includes a ***google shared link***(explained in the next paragraph) through canvas. This pdf file should be named as **Surname_Givenname_UNCCID_assignment*.pdf** (example: Jordan_Michael_800134567_assignment1.pdf for this assignment).\n",
    "\n",
    "To generate the ***google shared link***, first create a folder named ***Surname_Givenname_UNCCID_hw*.*** (example: Jordan_Michael_800134567_assignment1 for this assignment) in your Google Drive with your UNCC account. The structure of the files in the folder should be exactly the same as the one you downloaded. For instance in this homework:\n",
    "\n",
    "```\n",
    "Surname_Givenname_UNCCID_assignment1       \n",
    "        |---SourceImages\n",
    "        |---[Your_code].ipynb\n",
    "```\n",
    "Note that this folder should be in your Google Drive with your UNCC account.\n",
    "\n",
    "Then right click this folder, click ***Get shareable link***, in the People textfield, enter TA's emails: ***psingire@uncc.edu*** and ***kchiguru@uncc.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n",
    "\n",
    "Note that in google colab, we will only grade the version of the code right before the timestamp of the submission made in canvas. \n",
    "\n",
    "Extract the downloaded .zip file to a folder of your preference. The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_UNCCID_assignment1' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs.\n",
    "\n",
    "\n",
    "-- DO NOT change the folder structure, please just fill in the blanks. <br>\n",
    "\n",
    "You are encouraged to post and answer questions on Canvas. Please ask questions on Canvas and send emails only for personal issues.\n",
    "\n",
    "If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n",
    "\n",
    "Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work.\n",
    "\n",
    "Late submission penalty: <br>\n",
    "There will be a 10% penalty per day for late submission. However, you will have THREE days throughout the whole semester to submit late without penalty. Note that the grace period is calculated by days instead of hours. If you submit the homework one minute after the deadline, one late day will be counted. Likewise, if you submit one minute after the deadline, the 10% penaly will be imposed if not using the grace period.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
